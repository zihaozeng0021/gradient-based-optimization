@inproceedings{tato2018improving,
    title     = {Improving Adam Optimizer},
    author    = {Tato, Ange and Nkambou, Roger},
    booktitle = {Workshop Track at the 6th International Conference on Learning Representations (ICLR)},
    year      = {2018},
    url       = {https://openreview.net/forum?id=HJfpZq1DM}
}

@inproceedings{glorot2010understanding,
    title     = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
    author    = {Glorot, Xavier and Bengio, Yoshua},
    booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)},
    editor    = {Teh, Yee Whye and Titterington, Mike},
    volume    = {9},
    series    = {Proceedings of Machine Learning Research},
    pages     = {249--256},
    address   = {Chia Laguna Resort, Sardinia, Italy},
    publisher = {PMLR},
    year      = {2010},
    month     = {May},
    url       = {http://proceedings.mlr.press/v9/glorot10a.html}
}

@article{kingma2014adam,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Kingma, Diederik P. and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014},
  url     = {https://arxiv.org/abs/1412.6980}
}

@article{smith2017dont,
  title   = {Don't Decay the Learning Rate, Increase the Batch Size},
  author  = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  journal = {arXiv preprint arXiv:1711.00489},
  year    = {2017},
  url     = {https://arxiv.org/abs/1711.00489}
}

@article{you2019how,
  title   = {How Does Learning Rate Decay Help Modern Neural Networks?},
  author  = {You, Kaifeng and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I.},
  journal = {arXiv preprint arXiv:1908.01878},
  year    = {2019},
  url     = {https://arxiv.org/abs/1908.01878}
}

@article{robbins1951stochastic,
  title   = {A Stochastic Approximation Method},
  author  = {Robbins, Herbert and Monro, Sutton},
  journal = {The Annals of Mathematical Statistics},
  volume  = {22},
  number  = {3},
  pages   = {400--407},
  year    = {1951},
  publisher = {Institute of Mathematical Statistics},
  doi     = {10.1214/aoms/1177729586},
  url     = {https://projecteuclid.org/euclid.aoms/1177729586}
}

@book{burden2000numerical,
  title     = {Numerical Analysis},
  author    = {Burden, Richard L. and Faires, J. Douglas},
  edition   = {7th},
  publisher = {Brooks/Cole},
  year      = {2000},
  isbn      = {9780534382162}
}

@article{fu2023when,
  title   = {When and Why Momentum Accelerates SGD: An Empirical Study},
  author  = {Fu, Jingwen and Wang, Bohan and Zhang, Huishuai and Zhang, Zhizheng and Chen, Wei and Zheng, Nanning},
  journal = {arXiv preprint arXiv:2306.09000},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.09000}
}

@article{li2024surge,
  title   = {Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling},
  author  = {Li, Shuaipeng and Zhao, Penghao and Zhang, Hailin and others},
  journal = {arXiv preprint arXiv:2405.14578},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.14578}
}

@inproceedings{WilsonMartinez2003,
  author    = {Wilson, D. and Martinez, T.},
  title     = {The General Inefficiency of Batch Training for Gradient Descent Learning},
  booktitle = {Neural Networks},
  year      = {2003}
}
@book{GoodfellowBengioCourville2016,
  author    = {Goodfellow, I. and Bengio, Y. and Courville, A.},
  title     = {Deep Learning},
  year      = {2016},
  publisher = {MIT Press}
}

@inproceedings{MishkinSergievskiyMatas2017,
  author    = {Mishkin, D. and Sergievskiy, N. and Matas, J.},
  title     = {Systematic Evaluation of Convolution Neural Network Advances on the ImageNet},
  booktitle = {Computer Vision and Pattern Recognition Workshops},
  year      = {2017}
}

@inproceedings{KeskarEtAl2017,
  author    = {Keskar, N. S. and Mudigere, D. and Nocedal, J. and Smelyanskiy, M. and Tang, P. T. P.},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  booktitle = {International Conference on Learning Representations},
  year      = {2017}
}

@inproceedings{bottou2010large,
  author    = {Bottou, L\'{e}on},
  title     = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle = {Proceedings of COMPSTAT'2010: 19th International Conference on Computational Statistics},
  editor    = {Lechevallier, Yves and Saporta, Gilbert},
  pages     = {177--186},
  address   = {Paris, France},
  publisher = {Physica-Verlag HD},
  year      = {2010},
  doi       = {10.1007/978-3-7908-2604-3\_16},
  url       = {https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16}
}

@article{Frazier2018,
  author    = {Frazier, P. I.},
  title     = {A Tutorial on Bayesian Optimization},
  journal   = {arXiv preprint arXiv:1807.02811},
  year      = {2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2017},
  pages={5998--6008}
}

@article{Makinde2024,
  author    = {Makinde, A.},
  title     = {Optimizing Time Series Forecasting: A Comparative Study of Adam and Nesterov Accelerated Gradient on LSTM and GRU Networks Using Stock Market Data},
  journal   = {arXiv preprint arXiv:2410.01843},
  year      = {2024}
}

@inproceedings{AsadiEtAl2023,
  author    = {Asadi, K. and Fakoor, R. and Sabach, S.},
  title     = {Resetting the Optimizer in Deep RL: An Empirical Study},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {36},
  pages     = {72284--72324},
  year      = {2023}
}

